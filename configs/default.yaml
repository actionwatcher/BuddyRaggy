project:
  id: "demo"
  root_dir: "./rag_test/papers"
  index_dir: "./data/indices/ts"

ingestion:
  include_globs: [".py", ".cpp", ".h", ".md", ".pdf", ".txt"]
  exclude_globs: [".git", "node_modules"]

chunking:
  code:
    strategy: "tree_sitter"
    max_tokens: 320
    overlap_tokens: 32
  text:
    strategy: "semantic"
    max_tokens: 400
    overlap_tokens: 60

embedding:
  text_model: "intfloat/e5-base-v2"
  code_model: "intfloat/e5-base-v2"
  batch_size: 64
  device: "mps"   # "cpu" | "cuda" | "mps" | "auto"

vector_store:
  provider: "chroma"
  collection: "demo"

bm25:
  enabled: true

retrieval:
  top_k: 8
  alpha_dense: 0.7  # fusion weight: dense vs BM25
  rrf: true

llm:
  backend: "gemini"  # "ollama" | "openai"
  model: "gemini-2.0-flash" # llama3.1:8b | gemini-1.5 | gemini-2.0-flash
  max_output_tokens: 768
  temperature: 0.2

prompting:
  mode_default: "answer"
  system_message: |
    You are assisting in understanding documentation and code base. You are given set of sources that are extracted from vector store. Base you answer using only provided sources. Cite file paths.

security:
  offline_only: true
